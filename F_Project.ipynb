{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115c1db-d00d-4302-9707-7bd8d43a967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'C:\\Users\\adima\\Downloads\\cv-corpus-19.0-delta-2024-09-13-en\\cv-corpus-19.0-delta-2024-09-13\\en\\other.tsv', sep='\\t')\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e9634-0774-4afb-aa7e-ca1fbe9a4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where gender or age are missing\n",
    "data = data[data['gender'].isin(['female_feminine', 'male_masculine'])]\n",
    "data = data.dropna(subset=['gender', 'age'])\n",
    "data=data.drop([\"client_id\",\"sentence_id\",\"sentence\",\"sentence_domain\",\"up_votes\",\"down_votes\",\"accents\",\"variant\",\"locale\",\"segment\"],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3b0a3-048d-4800-828d-e5778907f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151271c-3a21-4e1f-b1d3-1552d9db6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a3d49-7c57-4c0a-8f9f-ae8410157ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['gender'].astype('category')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850735b-c224-4346-ad10-df0f64d1f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = data['age'].replace({\n",
    "    'teens': 'teens_twenties', \n",
    "    'twenties': 'teens_twenties',\n",
    "    'fifties': 'fifties_sixties', \n",
    "    'sixties': 'fifties_sixties'\n",
    "})\n",
    "data['age'].value_counts()\n",
    "# Keep only rows where 'path', 'age', and 'gender' are non-null\n",
    "data = data.dropna(subset=['path', 'age', 'gender'])\n",
    "\n",
    "# Check the result\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebb49e-966b-4be6-bc81-057387c1316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "def extract_features(audio_path):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    # Average MFCC over time to get a feature vector\n",
    "    mfcc = np.mean(mfcc.T, axis=0)\n",
    "    return mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee338e-81f9-49ce-a9a9-71df4831ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = r'C:\\Users\\adima\\Downloads\\cv-corpus-19.0-delta-2024-09-13-en\\cv-corpus-19.0-delta-2024-09-13\\en\\clips'\n",
    "features = []\n",
    "for index, row in data.iterrows():\n",
    "    # Get the file name from the 'path' column\n",
    "    file_name = row['path']\n",
    "    \n",
    "    # Combine directory path with file name to get the full path\n",
    "    audio_path = os.path.join(audio_dir, file_name)\n",
    "    features.append(extract_features(audio_path))\n",
    "    \n",
    "  \n",
    "# Convert the list of features to a NumPy array\n",
    "features = np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c15dd-12ff-4650-ab66-b5343d590559",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age']=data['age'].replace({\n",
    "                             'teens_twenties':0,\n",
    "                             'thirties':1,\n",
    "                             'fourties':2,\n",
    "                             'fifties_sixties':3})\n",
    "data['gender']=data['gender'].replace({\n",
    "                             'male_masculine':0,\n",
    "                             'female_feminine':1,})\n",
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54facd26-b8c2-4131-bc0d-160b05aa9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=data.drop([\"client_id\",\"sentence_id\",\"sentence\",\"sentence_domain\",\"up_votes\",\"down_votes\",\"accents\",\"variant\",\"locale\",\"segment\"],axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbb733-daf9-4004-9b0c-b501fb3c83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features  # Audio features\n",
    "y_gender = data['gender']  \n",
    "y_age = data['age']  \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a174a-aea8-4094-856a-c824dbd15b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1, y_gender_train1, y_gender_test1 = train_test_split(X, y_gender, test_size=0.2 ,random_state=42)\n",
    "X_train, X_test, y_age_train, y_age_test = train_test_split(X, y_age, test_size=0.2 ,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a93d84-7344-4585-a0a9-abaca8fdfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "gender_model = RandomForestClassifier(class_weight='balanced')\n",
    "gender_model.fit(X_train1, y_gender_train1)\n",
    "\n",
    "y_gender_pred1 = gender_model.predict(X_test1)\n",
    "print(\"Gender Classification Accuracy:\", accuracy_score(y_gender_test1, y_gender_pred1))\n",
    "\n",
    "cv_scores = cross_val_score(age_model, X_train1, y_gender_train1, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean Cross-validation Accuracy:\", np.mean(cv_scores))\n",
    "print(\"Standard Deviation of Cross-validation Accuracy:\", np.std(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0952b1-a53c-427e-9bcd-f73a9b4cc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model=RandomForestClassifier()\n",
    "age_model.fit(X_train,y_age_train)\n",
    "\n",
    "y_age_pred = age_model.predict(X_test)\n",
    "print(\"Age Classification Accuracy:\", accuracy_score(y_age_test, y_age_pred))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Perform cross-validation and calculate accuracy scores\n",
    "cv_scores = cross_val_score(age_model, X_train, y_age_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Output the cross-validation results\n",
    "print(\"Cross-validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean Cross-validation Accuracy:\", np.mean(cv_scores))\n",
    "print(\"Standard Deviation of Cross-validation Accuracy:\", np.std(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863fc5e-d37e-4d19-8083-b2cfadb3f0b3",
   "metadata": {},
   "source": [
    "Testing IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1877b-213e-4cfa-9fb2-10060910d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Set sample rate and duration\n",
    "sample_rate = 16000  # Match your model's sample rate\n",
    "duration = 5  # Seconds\n",
    "\n",
    "# Record audio\n",
    "print(\"Recording...\")\n",
    "audio_data = sd.rec(int(sample_rate * duration), samplerate=sample_rate, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording complete.\")\n",
    "\n",
    "# Save to file\n",
    "write(\"your_voice_sample.wav\", sample_rate, audio_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a85b15-9eb8-4d7a-a517-dcff84335ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=16000)  # Ensure correct sample rate\n",
    "\n",
    "    # Extract MFCCs or spectrogram (use the same features as used in training)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs = np.mean(mfccs.T, axis=0)  # Average MFCCs over time\n",
    "    return mfccs\n",
    "\n",
    "audio_features = preprocess_audio(\"your_voice_sample.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b0b22-770a-48e8-ba3a-cca277032b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape audio_features if necessary\n",
    "audio_features = audio_features.reshape(1, -1)  # Make it a 2D array for model input\n",
    "# Predict gender\n",
    "gender_prediction = gender_model.predict(audio_features)\n",
    "age_prediction = age_model.predict(audio_features)\n",
    "# Interpret the output\n",
    "gender = \"Male\" if gender_prediction[0] == 0 else \"Female\"  # or use actual labels if your model uses them\n",
    "# Define a mapping from numeric prediction to age category\n",
    "age_mapping = {\n",
    "    0: \"teens_twenties\",\n",
    "    1: \"thirties\",\n",
    "    2: \"fourties\",\n",
    "    3: \"fifties_sixties\"\n",
    "}\n",
    "\n",
    "# Get the predicted age category based on the prediction output\n",
    "age_prediction_value = age_prediction[0]\n",
    "age = age_mapping.get(age_prediction_value, \"Unknown\")  # Default to \"Unknown\" if the value is not in the mapping\n",
    "\n",
    "print(f\"Predicted Age Category: {age}\")\n",
    "\n",
    "print(f\"Predicted Gender: {gender}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b8055-7220-440b-a4d6-9ed033cc7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('age_model.pkl', 'wb') as file:\n",
    "    pickle.dump(age_model, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c417e3-f2c4-423a-9671-1483043de26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open('gender_model.pkl', 'wb') as file:\n",
    "    pickle.dump(age_model, file)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0875e-2549-44ee-99d5-97497fd33919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
